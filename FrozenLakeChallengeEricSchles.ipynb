{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake Challenge\n",
    "\n",
    "This notebook has been broken out into two parts:\n",
    "\n",
    "* An explaination of Q-Learning - the algorithm we'll be using\n",
    "* An various implementations of Q-Learning to solve the challenge\n",
    "\n",
    "## Understanding Reinforcement Learning In General\n",
    "\n",
    "The central idea of reinforcement learning, is to learn a strategy over time, based on interacting with an environment. There are strong correlaries between this notion of environment interaction and graph traversal. For this assignment I'll make use of Q learning to understand the basics of reinforcement learning. There are other techniques I could have employed, namely SARSA and Temporal difference learning, Policy Gradients, Deep Q-learning, Advantage Advantage Actor Critic, Proximal Policy Gradients, or Random Network Distillation.\n",
    "\n",
    "## Understanding Q-Learning In Specific\n",
    "\n",
    "The basic idea of Q learning is we start with a set of possible choices, and if we've seen all those choices before, we choose the choice that is the best (based on past experience).  The way the program evaluates which new possible choice is the best, is via looking at the rewards associated with each choice.  Let's look at a method to evaluate a set of choices, in the abstract:\n",
    "\n",
    "```\n",
    "def choose_action(state,actions,representation):\n",
    "    q = [evaluate(representation,(state,action)) for action in actions]\n",
    "    q = remove_all(q,None)\n",
    "    if q == []: return random.choice(actions)\n",
    "    else:\n",
    "        max_q = max(q)\n",
    "        num_equally_valued_actions = q.count(max_q)\n",
    "        if num_equally_valued_actions > 1:\n",
    "            possible_choices = [i for i in range(len(actions)) if q[i] == max_q]\n",
    "            action_choice = random.choice(possible_choices)\n",
    "        else:\n",
    "            action_choice = q.index(max_q)\n",
    "        return actions[action_choice]\n",
    "```\n",
    "\n",
    "Here state, represents the current state of the \"player\", the actions represent the possible actions, and the representation is the model rewards associated with each state,action pair.  Using the representation, the program chooses which action to execute next.  Here the representation is a dictionary, with keys as state,action pairs, and values as the associated rewards.  \n",
    "\n",
    "The code should be fairly straight forward: \n",
    "\n",
    "The full set of possible actions is observed and their associated rewards are recoded in a list: \n",
    "\n",
    "`q = [evaluate(representation,(state,action)) for action in actions]`\n",
    "\n",
    "This gives us a list \"q\" which gives us all the relevant rewards, indexed by possible actions.  We then choose the action which yields the greatest reward:\n",
    "\n",
    "`max_q = max(q)`\n",
    "\n",
    "If there are multiple rewards that yield the best possible scenario, we simply randomly choose one such reward.  If we have no information about our action set, we simply choose an action at random: \n",
    "\n",
    "`if q == []: return random.choice(actions)`\n",
    "\n",
    "This is more or else the crux of basic Q-Learning.  \n",
    "\n",
    "Now let's dig in a little bit to how our representation get's updated:\n",
    "\n",
    "```\n",
    "def update_representation(representation,state,action,reward,alpha=0.3):\n",
    "    try:\n",
    "        current_value = representation[(state,action)]\n",
    "    except:\n",
    "        current_value = None\n",
    "    if current_value:\n",
    "        representation[(state,action)] = current_value + alpha * reward\n",
    "    else:\n",
    "        representation[(state,action)] = reward\n",
    "    return representation\n",
    "```\n",
    "\n",
    "Here we do the most naive thing possible, simply discounting the present state against any past state.  In this way, our first impression dominates and any future iterations simply contribute to a lesser extent.  Over many repeated plays, this tends to cancel out and the long term trend will dominate eventually.\n",
    "\n",
    "### Playing with update_representation\n",
    "\n",
    "The long term representation that we use, after many iterations is very important to the learning mechanism we employ.  Intuitively, we can think of this long term representation as a `policy`.  By finding the right way to generate long term views, we can have significant gains in how our AI performs.  \n",
    "\n",
    "Let's look at some alternative ways to update our representation:\n",
    "\n",
    "```\n",
    "def update_representation(representation,counts,state,action,reward,alpha=0.3):\n",
    "    try:\n",
    "        current_value = representation[(state,action)]\n",
    "    except:\n",
    "        current_value = None\n",
    "    if current_value:\n",
    "    qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        representation[(state,action)] = current_value + (1/counts[(state,action)]*( reward - current_value ))\n",
    "        counts[(state,action)] += 1\n",
    "    else:\n",
    "        counts[(state,action)] = 1\n",
    "        representation[(state,action)] = reward\n",
    "    return representation,counts\n",
    "```\n",
    "\n",
    "In the above code instead of simply allowing our first value to dominate (until the long term), we use the average value of the representation.  Here the current value is the average up until this point and the somewhat confusing looking thing:\n",
    "\n",
    "`(1/counts[(state,action)]*( reward - current_value ))`\n",
    "\n",
    "Is the updated additive contribution of the reward: `reward - current_value`, discounted by the total number of terms seen `1/counts[(state,action)]`.  The reason that we need the count for each state,action combination is because we want the average for each state,action pair, rather than the overall state.  However, I assure you, this is the average.  \n",
    "\n",
    "A more natural, albeit less space efficient version of this code is below:\n",
    "\n",
    "```\n",
    "def update_representation(representation,counts,state,action,reward,alpha=0.3):\n",
    "    try:\n",
    "        current_value = representation[(state,action)]\n",
    "    except:\n",
    "        current_value = None\n",
    "    if current_value:\n",
    "\t    counts[(state,action)] += [reward]\n",
    "        representation[(state,action)] = sum(counts[(state,action)])/float(len(counts[(state,action)]))\n",
    "    else:\n",
    "        counts[(state,action)] = [reward]\n",
    "        representation[(state,action)] = reward\n",
    "    return representation,counts\n",
    "```\n",
    "\n",
    "## Let's look at the full code\n",
    "\n",
    "Now that we have everything all set up - let's look at how the train and play algorithms work in general.  We train the AI to play the game:\n",
    "\n",
    "```\n",
    "def train(board,iterations):\n",
    "    representation = {}\n",
    "    counts = {}\n",
    "    state = (0,0)\n",
    "    score = 0\n",
    "    actions = (\"up\",\"down\",\"left\",\"right\")\n",
    "    for _ in range(iterations):\n",
    "        while score < 100:\n",
    "            states = generate_new_board_states(board)\n",
    "            action = choose_action(state,actions,representation)\n",
    "            state = update_state(action,state,len(states),len(states[0]))\n",
    "            representation,counts = update_representation(representation,counts,state,action,states[state[0]][state[1]],alpha=0.3)\n",
    "            score = sum([elem for elem in representation.values() if elem > 0])\n",
    "    return representation\n",
    "```\n",
    "\n",
    "## Solving the Frozen Lake Challenge\n",
    "\n",
    "So far we've looked at a few ways of how to do Q-Learning, specifically a couple of ways we could update our reward over the long term.  Now let's go ahead and solve the problem in the frozen lake challenge - teaching an agent how to get over the frozen lake safely.  \n",
    "\n",
    "For this we'll make use of numpy, gym and random - as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Setting up the environment\n",
    "\n",
    "Since we are using the open gym environment, we'll set that up first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Setting up our Q-Table\n",
    "\n",
    "The formal name of the representation object we showed above is called a q-table.  The open gym framework tells us ahead of time how big our q-table should be.  So we can set it up ahead of time as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "qtable = np.zeros((state_size, action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Implementing Q-Learning In Numpy (instead of vanilla Python)\n",
    "\n",
    "Here we will make use of the policy based algorithm I've implemented above to train the agent.  You'll note that in this version we've added more tuning parameters to the policy engine - this is to improve performance in the model and to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.157054901123047\n",
      "Score 0.47833333333333333\n",
      "[[2.20187776e-01 7.34658517e-02 7.47754899e-02 4.79122658e-02]\n",
      " [8.73288881e-03 1.49991007e-02 9.48207847e-04 2.68411616e-01]\n",
      " [6.09828802e-03 7.66410342e-03 8.25722079e-03 2.40696396e-01]\n",
      " [2.03516613e-04 5.98210846e-03 1.73667862e-03 2.23190989e-02]\n",
      " [4.27998797e-01 7.54666868e-02 3.23511377e-02 4.28425720e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.07960956e-06 3.81691048e-09 1.08954583e-02 1.30138740e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.97529044e-02 6.91885817e-02 7.85186578e-02 5.90557999e-01]\n",
      " [1.11564972e-02 7.90597195e-01 1.19240052e-01 7.39234946e-02]\n",
      " [2.15434725e-01 8.42559502e-04 1.02602869e-04 3.43730764e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.99148855e-02 1.92397347e-02 8.59852709e-01 9.31978732e-02]\n",
      " [3.60315504e-01 9.89482037e-01 2.34672977e-01 1.53284346e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "def choose_action(qtable, state, epsilon):\n",
    "    if random.uniform(0, 1) > epsilon:\n",
    "        # exploitation based on previous knowledge\n",
    "        action = np.argmax(qtable[state,:])\n",
    "    else:\n",
    "        # exploration to avoid local maxima\n",
    "        action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def update_qtable(qtable, action, reward, state, new_state, learning_rate, alpha):\n",
    "    best_choice = alpha * np.max(qtable[new_state, :])\n",
    "    discount_rate = qtable[state, action]\n",
    "    reward_update = reward + best_choice - discount_rate\n",
    "    qtable[state, action] = qtable[state, action] + learning_rate * reward_update\n",
    "    return qtable\n",
    "\n",
    "def update_epsilon(min_epsilon, max_epsilon, decay_rate, episode):\n",
    "    return min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    \n",
    "def train_agent(qtable, total_episodes, learning_rate, max_steps, \n",
    "                alpha=0.95, epsilon=1.0, max_epsilon=1.0, min_epsilon=0.01, decay_rate=0.005):\n",
    "\n",
    "    rewards = []\n",
    "    for episode in range(total_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = choose_action(qtable, state, epsilon)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            qtable = update_qtable(qtable, action, reward,\n",
    "                                   state, new_state, learning_rate, alpha)\n",
    "            total_rewards += reward\n",
    "            state = new_state\n",
    "            if done == True: \n",
    "                break\n",
    "\n",
    "        # Reduce epsilon because we need less and less exploration\n",
    "        epsilon = update_epsilon(min_epsilon, max_epsilon, decay_rate, episode)\n",
    "        rewards.append(total_rewards)\n",
    "    return qtable, rewards\n",
    "\n",
    "total_episodes = 15000        \n",
    "learning_rate = 0.8           \n",
    "max_steps = 99                \n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "import time\n",
    "start = time.time()\n",
    "qtable, rewards = train_agent(qtable, total_episodes, learning_rate, max_steps, alpha=0.95, epsilon=1.0, max_epsilon=1.0, min_epsilon=0.01, decay_rate=0.005)\n",
    "print(time.time() - start)\n",
    "print (\"Score\", sum(rewards)/total_episodes)\n",
    "print(qtable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Our Agent's Parameters\n",
    "\n",
    "Now that we have a working agent, let's try to maximize our score by tuning some of the parameters we are passing into the `train_agent` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4921875\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "total_episodes = 16000        \n",
    "learning_rate_range = [0.8, 0.9]           \n",
    "max_steps_range = 110                \n",
    "alpha_range = [0.8, 0.9, 0.99] \n",
    "epsilon = 1.0\n",
    "max_epsilon=1.0 \n",
    "min_epsilon_range=[0.01, 0.05, 0.1] \n",
    "decay_rate_range=[0.001, 0.005, 0.01]\n",
    "\n",
    "qtables = []\n",
    "scores = []\n",
    "for parameters in itertools.product(learning_rate_range, alpha_range, min_epsilon_range, decay_rate_range):\n",
    "    \n",
    "    learning_rate = parameters[0]\n",
    "    alpha = parameters[1]\n",
    "    min_epsilon = parameters[2]\n",
    "    decay_rate = parameters[3]\n",
    "    \n",
    "    action_size = env.action_space.n\n",
    "    state_size = env.observation_space.n\n",
    "    qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "    qtable, rewards = train_agent(qtable, \n",
    "                                  total_episodes, \n",
    "                                  learning_rate, \n",
    "                                  max_steps, \n",
    "                                  alpha=0.95, \n",
    "                                  epsilon=1.0, \n",
    "                                  max_epsilon=1.0, \n",
    "                                  min_epsilon=0.01, \n",
    "                                  decay_rate=0.005)\n",
    "    scores.append(sum(rewards)/total_episodes)\n",
    "    qtables.append(qtable)\n",
    "    \n",
    "best_score = 0\n",
    "best_qtable = None\n",
    "for index, score in enumerate(scores):\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_qtable = qtables[index]\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's play the game!\n",
    "\n",
    "Now that we are done training our agent to be the best possible, we can play our game with the resultant agent, stored in the qtable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE  0\n",
      "\n",
      "EPISODE  1\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 48\n",
      "EPISODE  2\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 21\n",
      "EPISODE  3\n",
      "\n",
      "EPISODE  4\n",
      "\n",
      "EPISODE  5\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 16\n",
      "EPISODE  6\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 23\n",
      "EPISODE  7\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 21\n",
      "EPISODE  8\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 27\n",
      "EPISODE  9\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 54\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"EPISODE \", episode)\n",
    "    print()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        action = np.argmax(best_qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            env.render()\n",
    "            \n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
